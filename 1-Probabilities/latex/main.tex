%-----------------------Homework------------------------------------
%-------------------Arman Shokrollahi---------------------------------
%---------------------Coding Theory-------------------------------

\documentclass[a4 paper]{article}
% Set target color model to RGB
\usepackage[inner=1.5cm,outer=1.5cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage{setspace}
\usepackage[rgb]{xcolor}
\usepackage{verbatim}
\usepackage{amsgen,amsmath,amstext,amsbsy,amsopn,tikz,amssymb,tkz-linknodes}
\usepackage{fancyhdr}
\usepackage[colorlinks=true, urlcolor=blue,  linkcolor=blue, citecolor=blue]{hyperref}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{rotating}
%\usetikzlibrary{through,backgrounds}
\hypersetup{%
pdfauthor={Cristobal Donoso},%
pdftitle={Intro to Probabilities},%
pdfkeywords={Tikz,latex,bootstrap,uncertaintes},%
pdfcreator={PDFLaTeX},%
pdfproducer={PDFLaTeX},%
}
%\usetikzlibrary{shadows}
\usepackage[francais]{babel}
\usepackage{booktabs}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}

      \newtheorem{thm}{Theorem}[section]
      \newtheorem{prop}[thm]{Proposition}
      \newtheorem{lem}[thm]{Lemma}
      \newtheorem{cor}[thm]{Corollary}
      \newtheorem{defn}[thm]{Definition}
      \newtheorem{rem}[thm]{Remark}
      \numberwithin{equation}{section}

\newcommand{\homework}[6]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf 4171059-0: ~Data Science 1\hfill} }
       \vspace{6mm}
       \hbox to 6.28in { {\Large \hfill #1 (#2)  \hfill} }
       \vspace{6mm}
       \hbox to 6.28in { {\it Instructor: #3 \hfill Assistant: #5} }
       %\hbox to 6.28in { {\it TA: #4  \hfill #6}}
      \vspace{2mm}}
   }
   \end{center}
   \markboth{#5 -- #1}{#5 -- #1}
   \vspace*{4mm}
}

\newcommand{\bbF}{\mathbb{F}}
\newcommand{\bbX}{\mathbb{X}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bepsilon}{\boldsymbol{\epsilon}}
\newcommand{\balpha}{\boldsymbol{\alpha}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\0}{\mathbf{0}}

\begin{document}
\homework{Probabilities}{5/09/18 }{Guillermo Cabrera}{}{Cristobal Donoso}{}

\section*{Problem 1}
My neighbor has two children. Assuming that the gender of a child is like a coin flip, it is most likely, a priori, that my neighbor has one boy and one girl, with probability 1/2. The other possibilities -two boys or two girls- have probabilities 1/4 and 1/4.
\begin{itemize}
\item[a.] Suppose I ask him whether he has any boys, and he says yes. What is the probability that one child is a girl?
\item[b.] Suppose instead that I happen to see one of his children run by, and it is a boy. What is the probability that the other child is a girl?
\end{itemize}

\subsection*{Solution}
$ $\\
a.\\My neighbor has two children then we have 4 possible combinations. We define the sample space $\Omega$ as
\begin{equation}
\Omega = \{BB,BG,GB,GG\}
\end{equation}
where $B$ and $G$ is a boy and girl respectively. So the probability of one child being girl is $\frac{1}{2}$. If he said me that he has one any boys, then $\Omega$ changes,
\begin{equation}
\Omega = \{BB, BG, GB\}
\end{equation}
You're left with a $\frac{2}{3}$ probability that he has a girl. Formally,
\begin{equation}
P(G|B) = \frac{P(G\cap B)}{P(B)} = \frac{\frac{2}{3}\times1}{1} = \frac{2}{3}
\end{equation}
b.\\Now we have seen a boy. We only need to estimate the probability associated to the other child. So, the sample space is:
\begin{equation}
\Omega = \{B, G\}
\end{equation}
The probability related to the other child is $\frac{1}{2}$
%
%
%
%
%
\section*{Problem 2}
Show that the variance of a sum is VAR[$X+Y$] = VAR[$X$] + VAR[$Y$] + 2cov[$X$,$Y$] where cov[$X,Y$] is the covariance between $X$ and $Y$
\subsection*{Solution}
$ $\\
By definition
\begin{equation}\label{exp}
E[X+Y] = E[X]+ E[Y]
\end{equation}
\begin{equation}\label{var}
VAR[Z] = E[Z - E[Z]]^2
\end{equation}
\begin{equation}\label{var}
COV(X,Y) = E[(X - E[X])(Y - E[Y])]
\end{equation}
Let $Z = X + Y$ then \ref{var} becomes
\begin{equation}\label{var2}
VAR[X+Y] = E[(X+Y)-E[X+Y]]^2
\end{equation}
using \ref{exp} in \ref{var2}
\begin{equation}
VAR[X+Y] = E[(X+Y) - (E[X]+E[Y])]^2
\end{equation}
\begin{equation}
VAR[X+Y] = E[(X-E[X]) + (Y-E[Y])]^2
\end{equation}
\begin{equation}
VAR[X+Y] = E[(X-E[X])^2 +(Y-E[Y])^2]+ 2(X-E[X])(Y-E[Y])
\end{equation}
\begin{equation}
VAR[X+Y] = E[(X-E[X])^2]+E[(Y-E[Y])^2] + 2E[(X-E[X])(Y-E[Y])]
\end{equation}
Finally using \ref{var} properties
\begin{equation}
VAR[X+Y] = VAR[X] + VAR[Y] + 2COV(X,Y)
\end{equation} 
Notice that when we assume 0 covariance we have 
\begin{equation}
VAR[X+Y] = VAR[X] + VAR[Y]
\end{equation}
%
%
%
%
\section*{Problem 3}
Let H $\in$ $\{1,...,K\}$ be a discrete random variable, and let $e_1$ and $e_2$ be the observed values of two other random variables $E_1$ and $E_2$. Suppose we wish to calculate the vector
\begin{equation}
\overrightarrow{P}(e_1,e_2) = (P(H=1|e_1,e_2),...,P(H=K|e_1,e_2))
\end{equation}
Which of the following sets of numbers are sufficient for the calculation?
\begin{itemize}
\item[a.]\begin{itemize}
           \item[i.] $P(e_1,e_2), P(H), P(e_1|H), P(e_2|H)$
           \item[ii.] $P(e_1,e_2), P(H), P(e_1,e_2|H)$
           \item[iii.] $P(e_1|H), P(e_2|H), p(H)$
         \end{itemize}
\item[b.] Now suppose we now assume $E_1 \perp E_2|H$ (i.e $E_1$ and $E_2$ are conditionally independent given H), Which of the above 3 sets are sufficient now?
\end{itemize}
Show your calculations as well as giving the final results. Hint: use Bayes rule
\subsection*{Solution}
$ $\\
a.\\
We do not have any independence assumption between variables. Then, using Bayes rule 
\begin{equation}
P(H|e_1,e_2) = \frac{P(e_1,e_2|H)P(H)}{P(e_1,e_2)}
\end{equation}
The second set(ii.) have the numbers will we need.\\\\
b.\\
Now, we know that $P(E_1|H,E_2) = P(E_1, H)$. Using Bayes rule,
\begin{equation}
P(H|E_1,E_2) = \frac{P(H,E_1,E_2)}{P(E_1,E_2)}
\end{equation}
\begin{equation}
=\frac{P(E_1|H,E_2)P(H,E_2)}{P(E_1,E_2)}
\end{equation}
\begin{equation}
=\frac{P(E_1|H)P(E_2,H)}{P(E_1,E_2)}
\end{equation}
\begin{equation}
=\frac{P(E_1|H)P(E_2|H)P(H)}{P(E_1,E_2)}
\end{equation}
Thus we need to know the set i
%
%
%
%
\newpage
\section*{Problem 4}
After your yearly checkup, the doctor has bad news and good news. The bad news is that you tested positive for a serious disease and that the test is 99\% accurate (i.e., the probability of testing positive when you do have the disease is 0.99, as is the probability of testing negative when you donâ€™t have the disease). The good news is that it is a rare disease, striking only 1 in 10,000 people of your age. What is the probability that you actually have the disease? 
\subsection*{Solution}
Let define $X$ as the probability that you have the disease and $T$ the probability associated to the test accuracy. We need to calculate, 
\begin{equation}
P(X=1|T)
\end{equation}
Using the Bayes rule
\begin{equation}\label{ini}
P(X=1|T) = \frac{P(T|X)P(X)}{P(T)}
\end{equation}
Assuming the test accurate only depends on the disease's probability, we can derive $P(T)$ as the sum of all possibilities, namely
\begin{equation}\label{Pt}
P(T) = P(T|X=1)P(X=1)P(T|X=0)P(X=0)
\end{equation}
Then, using \ref{Pt} in \ref{ini}
\begin{equation}
P(X=1|T) = \frac{P(T|X=1)P(X=1)}{P(T|X=1)P(X=1)P(T|X=0)P(X=0)}
\end{equation}
\begin{equation}
P(X=1|T) = \frac{0.99\times 0.0001}{0.99\times 0.0001+0.01\times0.9999}
\end{equation}
\begin{equation}
\cong 0.009804
\end{equation}

\section*{Problem 5}
Verify that the Bernoulli distribution 
\begin{equation}\label{bern}
Bern(x|\mu) = \mu^x(1-\mu)^{1-x}
\end{equation}
satisfies the following properties 
\begin{equation}
\sum_{x=0}^1p(x|\mu) = 1
\end{equation}
\begin{equation}
E[x] = \mu
\end{equation}
\begin{equation}
var[x] = \mu(1-\mu)
\end{equation}
\subsection*{Solution}
$ $\\
From \ref{bern} we have
\begin{equation}
\sum_{x\in\{0,1\}}p(x|\mu) = p(x=0|\mu)+p(x=1|\mu)
\end{equation}
\begin{equation}
= (1-\mu)+\mu = 1
\end{equation}
\begin{equation}
\sum_{x\in\{0,1\}}xp(x|\mu) = 0\times p(x=0|\mu)+1\times p(x=1|\mu) = \mu
\end{equation}
\begin{equation}
\sum_{x\in \{0,1\}}(x-\mu)^2p(x|\mu) = \mu^2p(x=0|\mu) + (1-\mu)^2p(x=1|\mu)
\end{equation}
\begin{equation}
= \mu^2(1-\mu)+(1-\mu)^2\mu = \mu(1-\mu)
\end{equation}

\section*{Problem 6}
Consider the generalization of the squared loss function for a single target variable $t$ 
\begin{equation}
E[L] = \int\int\{y(x)-t\}^2p(x,t)dxdt
\end{equation}
to the case of multiple target variables described by the vector
\textbf{t} given by
\begin{equation}
E[L(\overrightarrow{t},y(x))] = \int\int ||y(x)-\overrightarrow{t}||^2p(x,\overrightarrow{t})dxd\overrightarrow{t}
\end{equation}
Using the calculus of variations, show that the function $y(x)$ for which this expected loss is minimized is given by $y(x) = E_t[\overrightarrow{t}|x]$. Show that this result reduces to
\begin{equation}
y(x) = \frac{\int tp(x,t)dt}{p(x)}= \int tp(t|x)dt = E_t[t|x]
\end{equation}
for the case of a single target variable t
\subsection*{Solution}
Our goal is to choose $y(x)$ so as to minimize $E[L]$. We can do it using, 
\begin{equation}
\frac{\partial E[L]}{\partial y(x)} = \int 2(y(x)-\overrightarrow{t})p(\overrightarrow{t},x)d\overrightarrow{t} = 0
\end{equation}
solving $y(x)$,
\begin{equation}
\int 2(y(x)-\overrightarrow{t})p(\overrightarrow{t},x)d\overrightarrow{t} = 0
\end{equation}
\begin{equation}
\int 2y(x)p(\overrightarrow{t},x)-2\overrightarrow{t}p(\overrightarrow{t},x)d\overrightarrow{t} = 0
\end{equation}
\begin{equation}
2y(x)\int p(\overrightarrow{t},x)d\overrightarrow{t}-2\int\overrightarrow{t}p(\overrightarrow{t},x)d\overrightarrow{t} = 0
\end{equation}
\begin{equation}\label{this}
y(x) = \frac{\int \overrightarrow{t} p(\overrightarrow{t},x)d\overrightarrow{t}}{\int p(\overrightarrow{t},x)d\overrightarrow{t}}
\end{equation}
by definition,
\begin{equation}\label{product}
P(x,y) = P(y|x)p(x)\qquad (product\ rule)
\end{equation}
\begin{equation}\label{sum}
P(x) = \sum_y P(x,y)\qquad  (sum\ rule)
\end{equation}
using \ref{product} and \ref{sum} on \ref{this}
\begin{equation}
y(x) = \frac{\int \overrightarrow{t} p(\overrightarrow{t}|x)p(x)d\overrightarrow{t}}{p(x)}
\end{equation}
\begin{equation}
y(x) = \frac{p(x)\int \overrightarrow{t} p(\overrightarrow{t}|x)d\overrightarrow{t}}{p(x)}
\end{equation}
\begin{equation}\label{last}
y(x) = \int \overrightarrow{t} p(\overrightarrow{t}|x)d\overrightarrow{t}
\end{equation}
\ref{last} represents the conditional average of $\overrightarrow{t}$ conditioned on $x$
\end{document}
%%------------ Arman Shokrollahi--------------%%
